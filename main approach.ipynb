{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a887f82-def7-4d29-94a5-f1f0f353f3fa",
   "metadata": {},
   "source": [
    "### Load the Model on GPU\n",
    "\n",
    "Here I am trying to load the model only on the second GPU of my server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c3163f1-26f5-484b-8012-0352127ff0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:03<00:00,  1.12it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Set CUDA device to 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "if torch.cuda.device_count() > 1:\n",
    "    torch.cuda.set_device(0)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load Qwen model\n",
    "model_name = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": device},  # Place on CUDA:0\n",
    "    trust_remote_code=True\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a69eae2-6da2-4a59-ba46-0f37c81e66cf",
   "metadata": {},
   "source": [
    "### Building contributor_skills.csv\n",
    "\n",
    "This dataset contains primary skills integrating for the same contributor. So, we have dataset for unique contributors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36faba5a-9dd0-4bd0-8642-8485b0328a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:03<00:00,  1.10it/s]\n",
      "Extracting Skills:   0%|                                                                                                            | 0/133 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:651: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Extracting Skills:  16%|███████████████▋                                                                                   | 21/133 [00:39<03:29,  1.87s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (39474 > 32768). Running this sequence through the model will result in indexing errors\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (32768). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "Extracting Skills: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 133/133 [05:45<00:00,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All rows processed. Final dataset saved as 'contributor_skills_simple.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from collections import defaultdict\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "contributor_skills = defaultdict(set)\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Extracting Skills\"):\n",
    "    contributor_id = row[\"contributor_id\"]\n",
    "    modified_source_files = str(row[\"modified_source_files\"])\n",
    "    commit_messages = str(row[\"commit_messages\"])\n",
    "\n",
    "    existing_skills_list = sorted(contributor_skills[contributor_id])\n",
    "    existing_skills_str = \"\\n\".join(f\"- {s}\" for s in existing_skills_list) if existing_skills_list else \"None\"\n",
    "\n",
    "    system_prompt = \"You are an AI assistant that analyzes code changes and commit messages to identify technical skills.\"\n",
    "    user_prompt = f\"\"\"Contributor's existing skills:\n",
    "    \n",
    "{existing_skills_str}\n",
    "\n",
    "New code changes:\n",
    "{modified_source_files}\n",
    "\n",
    "Commit message:\n",
    "{commit_messages}\n",
    "\n",
    "List only the new specific technical skills (not listed above) demonstrated in this change and commit message in bullet points.\n",
    "No extra descriptions or explanations.\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    chat_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    input_ids = tokenizer(chat_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **input_ids,\n",
    "            max_new_tokens=2048,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "    response_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    skills_output = response_text[len(chat_prompt):].strip()\n",
    "\n",
    "    new_skills = set()\n",
    "    for line in skills_output.splitlines():\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"-\"):\n",
    "            skill = line[1:].strip()\n",
    "            if skill:\n",
    "                new_skills.add(skill)\n",
    "\n",
    "    contributor_skills[contributor_id].update(new_skills)\n",
    "\n",
    "final_data = []\n",
    "for seq, (contributor_id, skills_set) in enumerate(contributor_skills.items(), start=1):\n",
    "    final_data.append({\n",
    "        \"sequence\": seq,\n",
    "        \"contributor_id\": contributor_id,\n",
    "        \"skills\": \", \".join(sorted(skills_set))\n",
    "    })\n",
    "\n",
    "output_df = pd.DataFrame(final_data)\n",
    "output_df.to_csv(\"contributor_skills.csv\", index=False)\n",
    "\n",
    "print(\"All rows processed. Final dataset saved as 'contributor_skills.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea781de-54b1-425d-965f-5ff154caa25a",
   "metadata": {},
   "source": [
    "### Apply only this stage was not good for accuracy, keeping for reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900b5868-b530-4952-85c2-b9f0bfb213f8",
   "metadata": {},
   "source": [
    "### Merging Skills based on Few Shots\n",
    "\n",
    "Tried merging repeating skills using few shots but the result was not good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97e570e-e6fc-4b34-a9ca-5810adc4e305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "df = pd.read_csv(\"contributor_skills.csv\")\n",
    "\n",
    "few_shot_prompt = \"\"\"You are a helpful assistant that cleans and merges technical skills in bullet points.\n",
    "\n",
    "Here are some examples:\n",
    "Example 1:\n",
    "Input Skills:\n",
    "- Used Bazel build system configuration\n",
    "- Demonstrated knowledge of build automation tools (Bazel)\n",
    "- Updated build templates for CUDA components\n",
    "\n",
    "Merged Skills:\n",
    "- Bazel\n",
    "- Build System Configuration\n",
    "\n",
    "Example 2:\n",
    "Input Skills:\n",
    "- Confirmed ability to manage project timelines and deliverables\n",
    "- Proven skill in contributing to successful and impactful software products\n",
    "- Proven skill in contributing to the development of high-quality software products\n",
    "\n",
    "Merged Skills:\n",
    "- Software Project Management\n",
    "\n",
    "Example 3:\n",
    "Input Skills:\n",
    "- Added detailed HLO operation profiles for NVIDIA B200 GPU\n",
    "- Added GPU spec for B200\n",
    "- Added autotuning results to fix gpu_compiler_test for Blackwell\n",
    "\n",
    "Merged Skills:\n",
    "- GPU Architecture (B200, Blackwell)\n",
    "- HLO Operation Profiling\n",
    "\n",
    "Example 4:\n",
    "Input Skills:\n",
    "- Highlighted experience with continuous integration and delivery pipelines\n",
    "- Showcased proficiency in automated testing and integration\n",
    "\n",
    "Merged Skills:\n",
    "- CI/CD Pipelines\n",
    "- Automated Testing\n",
    "\"\"\"\n",
    "\n",
    "merged_data = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Merging skills\"):\n",
    "    contributor_id = row[\"contributor_id\"]\n",
    "    sequence = row[\"sequence\"]\n",
    "    raw_skills = row[\"skills\"]\n",
    "\n",
    "    prompt = f\"\"\"{few_shot_prompt}\n",
    "\n",
    "Now merge the following skills for a contributor. Only merge contextually similar or identical skills. \n",
    "Do NOT remove distinct skills. Return the final cleaned and merged skill list in bullet points.\n",
    "\n",
    "Input Skills:\n",
    "{raw_skills}\n",
    "\n",
    "Merged Skills:\"\"\"\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **input_ids,\n",
    "            max_new_tokens=10000,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "    response_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    cleaned_skills_block = response_text[len(prompt):].strip()\n",
    "\n",
    "    merged_skills = []\n",
    "    for line in cleaned_skills_block.splitlines():\n",
    "        if line.strip().startswith(\"-\"):\n",
    "            skill = line.strip().lstrip(\"-•* \").strip()\n",
    "            if skill:\n",
    "                merged_skills.append(skill)\n",
    "\n",
    "    merged_data.append({\n",
    "        \"sequence\": sequence,\n",
    "        \"contributor_id\": contributor_id,\n",
    "        \"skills\": \", \".join(sorted(set(merged_skills)))\n",
    "    })\n",
    "\n",
    "merged_df = pd.DataFrame(merged_data)\n",
    "merged_df.to_csv(\"contributor_skills_merged.csv\", index=False)\n",
    "\n",
    "print(\"Merged skill set saved to 'contributor_skills_merged.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334c0473-faf1-4c96-a06f-83e425c676a0",
   "metadata": {},
   "source": [
    "### Merging Skills based on zero shots\n",
    "\n",
    "This time the result was better. So, it is the final dataset for contributors's skills dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "898d672d-f732-454d-8874-9ba08745f1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:03<00:00,  1.08it/s]\n",
      "Merging skills:   0%|                                                                                                                                  | 0/61 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:651: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Merging skills: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 61/61 [16:30<00:00, 16.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Merged skill set saved to 'contributor_skills_primary_merged.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "df = pd.read_csv(\"contributor_skills.csv\")\n",
    "\n",
    "merged_data = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Merging skills\"):\n",
    "    contributor_id = row[\"contributor_id\"]\n",
    "    sequence = row[\"sequence\"]\n",
    "    raw_skills = row[\"skills\"]\n",
    "\n",
    "\n",
    "    prompt = f\"\"\"Here is a list of technical skills for a contributor. Some of them are repeated in different wording or semantically similar.\n",
    "\n",
    "Please merge the contextually similar or identical skills into a cleaner form. Do NOT remove distinct skills.\n",
    "Return a cleaned, concise list of unique skills in bullet points (one skill per line).\n",
    "No full sentences, no duplicates.\n",
    "\n",
    "Input Skills:\n",
    "{raw_skills}\n",
    "\n",
    "Merged Skills:\"\"\"\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **input_ids,\n",
    "            max_new_tokens=2048,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "    response_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    cleaned_skills_block = response_text[len(prompt):].strip()\n",
    "\n",
    "    merged_skills = []\n",
    "    for line in cleaned_skills_block.splitlines():\n",
    "        if line.strip().startswith(\"-\"):\n",
    "            skill = line.strip().lstrip(\"-•* \").strip()\n",
    "            if skill:\n",
    "                merged_skills.append(skill)\n",
    "\n",
    "    merged_data.append({\n",
    "        \"sequence\": sequence,\n",
    "        \"contributor_id\": contributor_id,\n",
    "        \"skills\": \", \".join(sorted(set(merged_skills)))\n",
    "    })\n",
    "\n",
    "merged_df = pd.DataFrame(merged_data)\n",
    "merged_df.to_csv(\"contributor_skills_merged.csv\", index=False)\n",
    "\n",
    "print(\"Merged skill set saved to 'contributor_skills_primary_merged.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47b694a-7182-40da-98e5-c54afe40324d",
   "metadata": {},
   "source": [
    "### Important Note for Future Research: Please be aware that while numerous contributor_skills.csv files may have been observed previously, they are not available in the repository. The final contributor skills dataset, which achieved the optimal accuracy and is currently designated as contributor_skills.csv, was referred to as contributor_skills_primary_merged.csv in the preceding cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ab5382-f381-4750-8632-10883c9eb20e",
   "metadata": {},
   "source": [
    "### Creating Required Skills from Issue Title and Issue Description\n",
    "\n",
    "The dataset is okay but contains texts like assistant and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "109b8afd-2e86-4742-9b0b-e0a35af96980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting required skills:   0%|                                                                                                                            | 0/133 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:651: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Extracting required skills: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 133/133 [13:09<00:00,  5.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role-based skill extraction complete. Saved to 'issue_skills.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "issue_skills_data = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Extracting required skills\"):\n",
    "    issue_id = row[\"issue_id\"]\n",
    "    issue_title = str(row.get(\"issue_title\", \"\"))\n",
    "    issue_body = str(row[\"issue_body\"])\n",
    "\n",
    "    system_prompt = \"You are a software architect helping identify skills needed to resolve GitHub issues.\"\n",
    "    user_prompt = f\"\"\"Here is an issue's title and issue description.\n",
    "\n",
    "Issue Title:\n",
    "{issue_title}\n",
    "\n",
    "Issue Description:\n",
    "{issue_body}\n",
    "\n",
    "What are the technical skills required to solve this issue?\n",
    "\n",
    "### Response:\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    chat_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    inputs = tokenizer(chat_text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=5000,\n",
    "            do_sample=False,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "    full_response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    extracted_response = full_response.split(\"### Response:\")[-1].strip()\n",
    "\n",
    "    issue_skills_data.append({\n",
    "        \"issue_id\": issue_id,\n",
    "        \"required_skills\": extracted_response\n",
    "    })\n",
    "    \n",
    "output_df = pd.DataFrame(issue_skills_data)\n",
    "output_df.to_csv(\"issue_skills.csv\", index=False)\n",
    "\n",
    "print(\"Role-based skill extraction complete. Saved to 'issue_skills.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21a9c8c-85c1-439c-8527-18d08bec811a",
   "metadata": {},
   "source": [
    "### Cleaning the Datasets\n",
    "\n",
    "We clean the data and extracts only skills related texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "face080a-dd09-43d6-9c73-f41ef7b2c55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned skills saved to 'issue_skills.csv'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def extract_numbered_skills(text):\n",
    "    # Extract from first \"1.\" to end or before conclusion phrases\n",
    "    skill_block_match = re.search(r\"1\\..*?(?:(?:By possessing these skills)|(?:If you have any specific details)|$)\", text, re.DOTALL)\n",
    "    if not skill_block_match:\n",
    "        return \"\"\n",
    "    \n",
    "    skill_block = skill_block_match.group(0).strip()\n",
    "\n",
    "    # Keep only lines that start with numbers or bullet indicators\n",
    "    lines = skill_block.splitlines()\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if re.match(r\"^\\d+\\.\", line) or re.match(r\"^[-•*]\", line) or re.match(r\"^[-\\s]{2,}\", line):\n",
    "            cleaned_lines.append(line)\n",
    "\n",
    "    return \"\\n\".join(cleaned_lines)\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"issue_skills.csv\")\n",
    "df[\"required_skills\"] = df[\"required_skills\"].apply(extract_numbered_skills)\n",
    "\n",
    "df.to_csv(\"issue_skills.csv\", index=False)\n",
    "\n",
    "print(\"Cleaned skills saved to 'issue_skills.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd7a9034-1cab-491b-9912-e00b80523715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned file saved as 'issue_skills.csv' with 130 rows.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"issue_skills.csv\")\n",
    "\n",
    "# Drop NaN first, then drop rows where string is empty or only whitespace\n",
    "df_cleaned = df.dropna(subset=[\"required_skills\"])\n",
    "df_cleaned = df_cleaned[df_cleaned[\"required_skills\"].astype(str).str.strip() != \"\"]\n",
    "\n",
    "# Save cleaned file\n",
    "df_cleaned.to_csv(\"issue_skills.csv\", index=False)\n",
    "print(f\"Cleaned file saved as 'issue_skills.csv' with {len(df_cleaned)} rows.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b2260a-71c3-4d91-8021-d223fd71b99c",
   "metadata": {},
   "source": [
    "### Using LLM to find out Top Contributors\n",
    "\n",
    "We find the top_k contributors using LLM and as there is limit for input tokens, we use batch based technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62235390-5721-4a0e-9cbe-365531993fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring contributors:   0%|                                                                                                                             | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:651: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Scoring contributors: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:13<00:00, 13.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Contributors for Issue ID 24456:\n",
      "\n",
      "kasper0406 — Score: 85/100\n",
      "larsoner — Score: 85/100\n",
      "jreiffers — Score: 80/100\n",
      "dimvar — Score: 0/100\n",
      "sergachev — Score: 0/100\n",
      "jaro-sevcik — Score: 0/100\n",
      "acxz — Score: 0/100\n",
      "chaserileyroberts — Score: 0/100\n",
      "tyb0807 — Score: 0/100\n",
      "terryysun — Score: 0/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "issues_df = pd.read_csv(\"issue_skills.csv\")\n",
    "contributors_df = pd.read_csv(\"contributor_skills.csv\")\n",
    "\n",
    "max_token_limit = 30000\n",
    "top_k = 10\n",
    "\n",
    "issue_row = issues_df.iloc[0]\n",
    "issue_id = issue_row[\"issue_id\"]\n",
    "required_skills = str(issue_row[\"required_skills\"])\n",
    "\n",
    "batches = []\n",
    "current_batch = []\n",
    "current_token_count = 0\n",
    "\n",
    "for idx, row in contributors_df.iterrows():\n",
    "    cid = row[\"contributor_id\"]\n",
    "    skills = str(row[\"skills\"])\n",
    "    entry = f\"Contributor ID: {cid}\\nSkills:\\n{skills}\\n\\n\"\n",
    "    tokens = len(tokenizer(entry)[\"input_ids\"])\n",
    "    \n",
    "    # Start a new batch if adding the current entry exceeds token limit\n",
    "    if current_token_count + tokens > max_token_limit:\n",
    "        batches.append(current_batch)\n",
    "        current_batch = []\n",
    "        current_token_count = 0\n",
    "    \n",
    "    current_batch.append((cid, skills))\n",
    "    current_token_count += tokens\n",
    "\n",
    "if current_batch:\n",
    "    batches.append(current_batch)\n",
    "\n",
    "valid_ids = set(contributors_df[\"contributor_id\"].astype(str))\n",
    "contributor_scores = {}\n",
    "\n",
    "for batch in tqdm(batches, desc=\"Scoring contributors\"):\n",
    "    # Create a single input text block with all contributors in the batch\n",
    "    contributor_block = \"\\n\".join(\n",
    "        f\"Contributor ID: {cid}\\nSkills:\\n{skills}\" for cid, skills in batch\n",
    "    )\n",
    "\n",
    "    prompt_text = f\"\"\"You are a helpful assistant that recommends contributors for GitHub issues.\n",
    "\n",
    "Given the following required skills for an issue:\n",
    "\n",
    "{required_skills}\n",
    "\n",
    "Below is a list of contributors and their skills:\n",
    "\n",
    "{contributor_block}\n",
    "\n",
    "Please rate each contributor from 0 to 100 based on how suitable they are for solving the issue.\n",
    "\n",
    "Return the output in this format:\n",
    "contributor_id: score\n",
    "\n",
    "Only return one contributor per line. Do not include explanations.\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Run the model in inference mode without gradient computation\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1000,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    full_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    response = full_output.split(\"### Response:\")[-1].strip()\n",
    "\n",
    "    for line in response.splitlines():\n",
    "        if \":\" in line:\n",
    "            parts = line.split(\":\", 1)\n",
    "            cid = parts[0].strip()\n",
    "            try:\n",
    "                score = int(parts[1].strip())\n",
    "                # Only record valid contributor IDs with scores in the correct range\n",
    "                if cid in valid_ids and 0 <= score <= 100:\n",
    "                    contributor_scores[cid] = score\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "top_contributors = sorted(contributor_scores.items(), key=lambda x: -x[1])[:top_k]\n",
    "\n",
    "print(f\"\\nTop {top_k} Contributors for Issue ID {issue_id}:\\n\")\n",
    "for cid, score in top_contributors:\n",
    "    print(f\"{cid} — Score: {score}/100\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafdd4e3-7fe5-4750-bfcf-097a286006b3",
   "metadata": {},
   "source": [
    "### Finding top_k Accuracy for LLM based Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "24eb1ed5-1b87-4d34-ac33-f9ba1608df6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:04<00:00,  1.24s/it]\n",
      "Evaluating Issues:   0%|                                                                                                                              | 0/133 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:651: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Evaluating Issues: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 133/133 [27:58<00:00, 12.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-15 Accuracy: 0.2331 (31/133)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load datasets\n",
    "dataset_df = pd.read_csv(\"dataset.csv\")\n",
    "issues_df = pd.read_csv(\"issue_skills.csv\")\n",
    "contributors_df = pd.read_csv(\"contributor_skills.csv\")\n",
    "\n",
    "issue_skill_map = dict(zip(issues_df[\"issue_id\"], issues_df[\"required_skills\"].astype(str)))\n",
    "contributor_skill_map = dict(zip(contributors_df[\"contributor_id\"], contributors_df[\"skills\"].astype(str)))\n",
    "\n",
    "top_k = 15\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for _, row in tqdm(dataset_df.iterrows(), total=len(dataset_df), desc=\"Evaluating Issues\"):\n",
    "    issue_id = row[\"issue_id\"]\n",
    "    true_contributor = row[\"contributor_id\"]\n",
    "\n",
    "    if issue_id not in issue_skill_map:\n",
    "        continue\n",
    "\n",
    "    required_skills = issue_skill_map[issue_id]\n",
    "\n",
    "    # Prepare batches (same logic as before)\n",
    "    max_token_limit = 30000\n",
    "    batches = []\n",
    "    current_batch = []\n",
    "    current_token_count = 0\n",
    "\n",
    "    for cid, skills in contributor_skill_map.items():\n",
    "        entry = f\"Contributor ID: {cid}\\nSkills:\\n{skills}\\n\\n\"\n",
    "        tokens = len(tokenizer(entry)[\"input_ids\"])\n",
    "        if current_token_count + tokens > max_token_limit:\n",
    "            batches.append(current_batch)\n",
    "            current_batch = []\n",
    "            current_token_count = 0\n",
    "        current_batch.append((cid, skills))\n",
    "        current_token_count += tokens\n",
    "    if current_batch:\n",
    "        batches.append(current_batch)\n",
    "\n",
    "    contributor_scores = {}\n",
    "\n",
    "    for batch in batches:\n",
    "        contributor_block = \"\\n\".join(\n",
    "            f\"Contributor ID: {cid}\\nSkills:\\n{skills}\" for cid, skills in batch\n",
    "        )\n",
    "\n",
    "        prompt_text = f\"\"\"You are a helpful assistant that recommends contributors for GitHub issues.\n",
    "\n",
    "Given the following required skills for an issue:\n",
    "\n",
    "{required_skills}\n",
    "\n",
    "Below is a list of contributors and their skills:\n",
    "\n",
    "{contributor_block}\n",
    "\n",
    "Please rate each contributor from 0 to 100 based on how suitable they are for solving the issue.\n",
    "\n",
    "Return the output in this format:\n",
    "contributor_id: score\n",
    "\n",
    "Only return one contributor per line. Do not include explanations.\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "        inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=1500,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        full_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        response = full_output.split(\"### Response:\")[-1].strip()\n",
    "\n",
    "        for line in response.splitlines():\n",
    "            if \":\" in line:\n",
    "                parts = line.split(\":\", 1)\n",
    "                cid = parts[0].strip()\n",
    "                try:\n",
    "                    score = int(parts[1].strip())\n",
    "                    if 0 <= score <= 100:\n",
    "                        contributor_scores[cid] = score\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "    # Evaluate top_k\n",
    "    total += 1\n",
    "    top_contributors = sorted(contributor_scores.items(), key=lambda x: -x[1])[:top_k]\n",
    "    top_ids = [cid for cid, _ in top_contributors]\n",
    "    if true_contributor in top_ids:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / total if total > 0 else 0.0\n",
    "print(f\"\\nTop-{top_k} Accuracy: {accuracy:.4f} ({correct}/{total})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0356ef0b-4064-471a-9c0d-2f73a1ada542",
   "metadata": {},
   "source": [
    "Top-10 Accuracy: 0.1955 (26/133)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19490d0-9cf9-4194-aff0-c6671a4f07b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225e90ff-2cef-432b-b953-960e6b11f5bf",
   "metadata": {},
   "source": [
    "### Using s-BERT to find out top contributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf495f18-e1e7-43c3-9d63-1a11e0073fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n",
      "\n",
      "Top 15 contributors for Issue ID 24456:\n",
      "\n",
      "sergachev — Score: 0.6174\n",
      "agriyakhetarpal — Score: 0.6109\n",
      "kasper0406 — Score: 0.6051\n",
      "ngoldbaum — Score: 0.6002\n",
      "yakovdan — Score: 0.5892\n",
      "terryysun — Score: 0.5863\n",
      "chunhsue — Score: 0.5806\n",
      "charris — Score: 0.5745\n",
      "tensorflower-gardener — Score: 0.5719\n",
      "baskargopinath — Score: 0.5441\n",
      "yliu120 — Score: 0.5261\n",
      "unknown — Score: 0.5249\n",
      "HaoZeke — Score: 0.5172\n",
      "wheeleha — Score: 0.4947\n",
      "mayeut — Score: 0.4944\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "issue_df = pd.read_csv(\"issue_skills.csv\")\n",
    "contributor_df = pd.read_csv(\"contributor_skills.csv\")\n",
    "\n",
    "issue_row = issue_df.iloc[0]\n",
    "issue_id = issue_row[\"issue_id\"]\n",
    "required_skills_text = str(issue_row[\"required_skills\"])\n",
    "\n",
    "contributor_ids = contributor_df[\"contributor_id\"].tolist()\n",
    "contributor_skills = contributor_df[\"skills\"].astype(str).tolist()\n",
    "\n",
    "issue_embedding = model.encode(required_skills_text, convert_to_tensor=True)\n",
    "contributor_embeddings = model.encode(contributor_skills, convert_to_tensor=True)\n",
    "\n",
    "#cosine similarity on s-BERT embeddings\n",
    "cosine_scores = util.cos_sim(issue_embedding, contributor_embeddings)[0]\n",
    "\n",
    "top_k = 15\n",
    "top_results = torch.topk(cosine_scores, k=top_k)\n",
    "\n",
    "print(f\"\\nTop {top_k} contributors for Issue ID {issue_id}:\\n\")\n",
    "for score, idx in zip(top_results.values, top_results.indices):\n",
    "    print(f\"{contributor_ids[idx]} — Score: {score.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c11a89-c85b-4706-b079-3ed74016b417",
   "metadata": {},
   "source": [
    "### Finding top_k Accuracy for s-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e660c697-918c-441e-b52b-29692ae8fc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-15 s-BERT Accuracy: 60.00 % (78/130)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "dataset_df = pd.read_csv(\"dataset.csv\")\n",
    "issue_df = pd.read_csv(\"issue_skills.csv\")\n",
    "contributor_df = pd.read_csv(\"contributor_skills.csv\")\n",
    "\n",
    "issue_skill_map = dict(zip(issue_df[\"issue_id\"], issue_df[\"required_skills\"].astype(str)))\n",
    "contributor_skill_map = dict(zip(contributor_df[\"contributor_id\"], contributor_df[\"skills\"].astype(str)))\n",
    "\n",
    "top_k = 15\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "for _, row in dataset_df.iterrows():\n",
    "    issue_id = row[\"issue_id\"]\n",
    "    true_contributor = row[\"contributor_id\"]\n",
    "\n",
    "    if issue_id not in issue_skill_map or true_contributor not in contributor_skill_map:\n",
    "        continue\n",
    "\n",
    "    required_skills_text = issue_skill_map[issue_id]\n",
    "\n",
    "    contributor_ids = list(contributor_skill_map.keys())\n",
    "    contributor_skills = list(contributor_skill_map.values())\n",
    "\n",
    "    issue_embedding = model.encode(required_skills_text, convert_to_tensor=True)\n",
    "    contributor_embeddings = model.encode(contributor_skills, convert_to_tensor=True)\n",
    "\n",
    "    cosine_scores = util.cos_sim(issue_embedding, contributor_embeddings)[0]\n",
    "    top_results = torch.topk(cosine_scores, k=top_k)\n",
    "\n",
    "    top_contributor_ids = [contributor_ids[idx] for idx in top_results.indices.tolist()]\n",
    "    total += 1\n",
    "    if true_contributor in top_contributor_ids:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / total if total > 0 else 0.0\n",
    "accuracy = accuracy*100\n",
    "print(f\"\\nTop-{top_k} s-BERT Accuracy: {accuracy:.2f} % ({correct}/{total})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279e91d5-13e4-4df5-a339-b1bf7893c1db",
   "metadata": {},
   "source": [
    "### Using TF-IDF for Checking Top Contributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc85ec3b-0a13-42c4-82ab-032c35378a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 TF-IDF Contributors for Issue ID 24456:\n",
      "\n",
      "apivovarov — Score: 0.3445\n",
      "charris — Score: 0.3336\n",
      "ngoldbaum — Score: 0.3137\n",
      "kasper0406 — Score: 0.2856\n",
      "mhvk — Score: 0.2648\n",
      "yakovdan — Score: 0.2635\n",
      "mayeut — Score: 0.2439\n",
      "terryysun — Score: 0.2326\n",
      "jreiffers — Score: 0.2303\n",
      "ArvidJB — Score: 0.2273\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "issues_df = pd.read_csv(\"issue_skills.csv\")\n",
    "contributors_df = pd.read_csv(\"contributor_skills.csv\")\n",
    "\n",
    "issue_row = issues_df.iloc[0]\n",
    "issue_id = issue_row[\"issue_id\"]\n",
    "required_skills = str(issue_row[\"required_skills\"])\n",
    "\n",
    "contributor_ids = contributors_df[\"contributor_id\"].tolist()\n",
    "contributor_skills = contributors_df[\"skills\"].astype(str).tolist()\n",
    "\n",
    "corpus = [required_skills] + contributor_skills\n",
    "\n",
    "# using TF-IDF to generate embeddings\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "#cosine similarity on generated embeddings\n",
    "cosine_scores = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:]).flatten()\n",
    "\n",
    "top_k = 10\n",
    "top_indices = cosine_scores.argsort()[::-1][:top_k]\n",
    "\n",
    "print(f\"\\nTop {top_k} TF-IDF Contributors for Issue ID {issue_id}:\\n\")\n",
    "for idx in top_indices:\n",
    "    cid = contributor_ids[idx]\n",
    "    score = cosine_scores[idx]\n",
    "    print(f\"{cid} — Score: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ce1d631-0d10-40f6-b27d-31a36d8bc00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bottom 10 TF-IDF Contributors for Issue ID 24456:\n",
      "\n",
      "SaraInCode — Score: 0.0000\n",
      "StanFromIreland — Score: 0.0000\n",
      "sterrettm2 — Score: 0.0000\n",
      "setbit123 — Score: 0.0000\n",
      "DWesl — Score: 0.0067\n",
      "acxz — Score: 0.0152\n",
      "jiunkaiy — Score: 0.0222\n",
      "hauntsaninja — Score: 0.0238\n",
      "philipphack — Score: 0.0242\n",
      "Tixxx — Score: 0.0344\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "issues_df = pd.read_csv(\"issue_skills.csv\")\n",
    "contributors_df = pd.read_csv(\"contributor_skills.csv\")\n",
    "\n",
    "issue_row = issues_df.iloc[0]\n",
    "issue_id = issue_row[\"issue_id\"]\n",
    "required_skills = str(issue_row[\"required_skills\"])\n",
    "\n",
    "contributor_ids = contributors_df[\"contributor_id\"].tolist()\n",
    "contributor_skills = contributors_df[\"skills\"].astype(str).tolist()\n",
    "\n",
    "corpus = [required_skills] + contributor_skills\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "cosine_scores = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:]).flatten()\n",
    "\n",
    "bottom_k = 10\n",
    "bottom_indices = cosine_scores.argsort()[:bottom_k]  # No [::-1] = lowest scores\n",
    "\n",
    "print(f\"\\nBottom {bottom_k} TF-IDF Contributors for Issue ID {issue_id}:\\n\")\n",
    "for idx in bottom_indices:\n",
    "    cid = contributor_ids[idx]\n",
    "    score = cosine_scores[idx]\n",
    "    print(f\"{cid} — Score: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f966a2a6-1062-4414-9edf-7ea260b6ac91",
   "metadata": {},
   "source": [
    "### Finding Accuracy of TF-IDF based top_k Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eac8b495-efe7-479a-a1ce-e0963b4c5a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-15 TF-IDF Accuracy: 70.00 % (91/130)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "dataset_df = pd.read_csv(\"dataset.csv\")\n",
    "issues_df = pd.read_csv(\"issue_skills.csv\")\n",
    "contributors_df = pd.read_csv(\"contributor_skills.csv\")\n",
    "\n",
    "# Convert maps for fast lookup\n",
    "issue_skill_map = dict(zip(issues_df[\"issue_id\"], issues_df[\"required_skills\"].astype(str)))\n",
    "contributor_skill_map = dict(zip(contributors_df[\"contributor_id\"], contributors_df[\"skills\"].astype(str)))\n",
    "\n",
    "top_k = 15\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "for _, row in dataset_df.iterrows():\n",
    "    issue_id = row[\"issue_id\"]\n",
    "    true_contributor = row[\"contributor_id\"]\n",
    "\n",
    "    if issue_id not in issue_skill_map or true_contributor not in contributor_skill_map:\n",
    "        continue\n",
    "\n",
    "    required_skills = issue_skill_map[issue_id]\n",
    "    if pd.isna(required_skills) or required_skills.strip() == \"\":\n",
    "        continue\n",
    "\n",
    "    # Build valid contributor pool (skip 'No significant skills found.')\n",
    "    contributor_ids = []\n",
    "    contributor_texts = []\n",
    "    for cid, skills in contributor_skill_map.items():\n",
    "        if pd.isna(skills) or skills.strip() == \"No significant skills found.\":\n",
    "            continue\n",
    "        contributor_ids.append(cid)\n",
    "        contributor_texts.append(skills)\n",
    "\n",
    "    # Skip if contributor pool is empty or true_contributor was filtered out\n",
    "    if not contributor_ids or true_contributor not in contributor_ids:\n",
    "        continue\n",
    "\n",
    "    # TF-IDF vectorization and cosine similarity\n",
    "    corpus = [required_skills.strip()] + contributor_texts\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    cosine_scores = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:]).flatten()\n",
    "    top_indices = cosine_scores.argsort()[::-1][:top_k]\n",
    "    top_contributors = [contributor_ids[i] for i in top_indices]\n",
    "\n",
    "    total += 1\n",
    "    if true_contributor in top_contributors:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = (correct / total) * 100 if total > 0 else 0.0\n",
    "print(f\"\\nTop-{top_k} TF-IDF Accuracy: {accuracy:.2f} % ({correct}/{total})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a80fae9-88c8-4134-a14a-7cb8cb4cc9fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
