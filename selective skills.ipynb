{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76527aea",
   "metadata": {},
   "source": [
    "### First we load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719328d2-f9c5-456c-8deb-ed6535d16ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:04<00:00,  1.06s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
    "    trust_remote_code=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc7b898",
   "metadata": {},
   "source": [
    "### We are trying to build a superset of skills that will contain all the skills in our dataset of the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbd16273-ffc5-4488-a455-2a8b92f94afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting skills:   0%|                                                                                                            | 0/133 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:651: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Extracting skills:  16%|███████████████▋                                                                                   | 21/133 [02:10<10:58,  5.88s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (39456 > 32768). Running this sequence through the model will result in indexing errors\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (32768). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "Extracting skills: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 133/133 [14:08<00:00,  6.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Skill superset saved to 'skill_superset.txt' with 348 unique skills.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"dataset.csv\")\n",
    "all_skills = set()\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Extracting skills\"):\n",
    "    issue_title = str(row.get(\"issue_title\", \"\"))\n",
    "    issue_body = str(row.get(\"issue_body\", \"\"))\n",
    "    modified_source = str(row.get(\"modified_source_files\", \"\"))\n",
    "    commit_message = str(row.get(\"commit_messages\", \"\"))\n",
    "\n",
    "    prompt = f\"\"\"You are an AI assistant that extracts technical skills from GitHub-related development activity.\n",
    "\n",
    "Analyze the following information and list the specific technical skills involved:\n",
    "\n",
    "Issue Title:\n",
    "{issue_title}\n",
    "\n",
    "Issue Description:\n",
    "{issue_body}\n",
    "\n",
    "Modified Source Files:\n",
    "{modified_source}\n",
    "\n",
    "Commit Message:\n",
    "{commit_message}\n",
    "\n",
    "Return only the technical skills in bullet points. Avoid soft skills and generalities.\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    skills_block = response.split(\"### Response:\")[-1].strip()\n",
    "\n",
    "    for line in skills_block.splitlines():\n",
    "        if line.strip().startswith(\"-\"):\n",
    "            skill = line.strip().lstrip(\"-•* \").strip()\n",
    "            if skill:\n",
    "                all_skills.add(skill)\n",
    "\n",
    "# Save to a .txt file with semicolon separation\n",
    "with open(\"skill_superset.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"; \".join(sorted(all_skills)))\n",
    "\n",
    "print(f\"\\nSkill superset saved to 'skill_superset.txt' with {len(all_skills)} unique skills.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877c302b",
   "metadata": {},
   "source": [
    "### Now we will use that list as baseline and try to build the two dataset like the main approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7496a4-46cc-465c-9f1f-8accb5b37f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting contributor skills:   0%|                                                                                                | 0/133 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:651: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Extracting contributor skills:  16%|█████████████▋                                                                         | 21/133 [01:04<05:08,  2.75s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (41361 > 32768). Running this sequence through the model will result in indexing errors\n",
      "Extracting contributor skills: 100%|██████████████████████████████████████████████████████████████████████████████████████| 133/133 [33:23<00:00, 15.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved to 'selective_contributor_skill.csv' with 61 contributors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load main dataset\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "# Load canonical skill list\n",
    "with open(\"skill_superset.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    canonical_skills = [s.strip() for s in f.read().split(\";\") if s.strip()]\n",
    "baseline_block = \"\\n\".join(f\"- {s}\" for s in sorted(set(canonical_skills)))\n",
    "\n",
    "# Dict that accumulates skills across multiple rows per contributor\n",
    "contributor_skills = defaultdict(set)\n",
    "\n",
    "# Process each row\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Extracting contributor skills\"):\n",
    "    contributor_id = row[\"contributor_id\"]\n",
    "    modified_source_files = str(row.get(\"modified_source_files\", \"\"))\n",
    "    commit_messages = str(row.get(\"commit_messages\", \"\"))\n",
    "\n",
    "    system_prompt = \"You are an expert assistant that extracts technical skills from GitHub commits based on a canonical skill list.\"\n",
    "\n",
    "    user_prompt = f\"\"\"Here is a list of allowed canonical technical skills:\n",
    "{baseline_block}\n",
    "\n",
    "Now analyze the following contribution:\n",
    "\n",
    "Modified Source Files:\n",
    "{modified_source_files}\n",
    "\n",
    "Commit Message:\n",
    "{commit_messages}\n",
    "\n",
    "Return only the relevant skills from the above list in bullet points. No explanation.\n",
    "\"\"\"\n",
    "\n",
    "    # Format as chat template\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    chat_input = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(chat_input, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate model response\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=4096,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    response_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    skills_block = response_text[len(chat_input):].strip()\n",
    "\n",
    "    for line in skills_block.splitlines():\n",
    "        if line.strip().startswith(\"-\"):\n",
    "            skill = line.strip().lstrip(\"-•* \").strip()\n",
    "            if skill:\n",
    "                contributor_skills[contributor_id].add(skill)\n",
    "\n",
    "# Build final DataFrame\n",
    "final_data = []\n",
    "all_contributors = df[\"contributor_id\"].unique()\n",
    "\n",
    "for seq, cid in enumerate(all_contributors, start=1):\n",
    "    skills = contributor_skills.get(cid)\n",
    "    if not skills:\n",
    "        skill_text = \"No significant skills found.\"\n",
    "    else:\n",
    "        skill_text = \", \".join(sorted(skills))\n",
    "\n",
    "    final_data.append({\n",
    "        \"sequence\": seq,\n",
    "        \"contributor_id\": cid,\n",
    "        \"skills\": skill_text\n",
    "    })\n",
    "\n",
    "output_df = pd.DataFrame(final_data)\n",
    "output_df.to_csv(\"selective_contributor_skills.csv\", index=False)\n",
    "\n",
    "print(f\"\\nSaved to 'selective_contributor_skill.csv' with {len(output_df)} contributors.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1424e6",
   "metadata": {},
   "source": [
    "### Now we filter out those contributors who have no significant skills in terms of this repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c8f7511-5b8a-4ebf-9832-6de94f205682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved filtered dataset to 'selective_contributor_skills_filtered.csv' with 56 contributors.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"selective_contributor_skills.csv\")\n",
    "\n",
    "# Filter out rows with \"No significant skills found.\"\n",
    "df_filtered = df[df[\"skills\"].str.strip() != \"No significant skills found.\"]\n",
    "\n",
    "df_filtered.to_csv(\"selective_contributor_skills_filtered.csv\", index=False)\n",
    "\n",
    "print(f\"Saved filtered dataset to 'selective_contributor_skills_filtered.csv' with {len(df_filtered)} contributors.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8515005",
   "metadata": {},
   "source": [
    "### Now it's time to extract required skills for issues like the main approach but using canonical superset skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae52a34-d33a-44ad-9e8f-62e0e9e80057",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:03<00:00,  1.10it/s]\n",
      "Extracting issue skills:   0%|                                                                                                      | 0/133 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:651: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Extracting issue skills: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 133/133 [18:39<00:00,  8.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved filtered issue skill dataset to 'selective_issue_skills.csv' with 127 issues.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "# Load canonical skill list\n",
    "with open(\"skill_superset.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    canonical_skills = [s.strip() for s in f.read().split(\";\") if s.strip()]\n",
    "baseline_block = \"\\n\".join(f\"- {s}\" for s in sorted(set(canonical_skills)))\n",
    "\n",
    "# Load model/tokenizer although it is not required as loaded earlier\n",
    "model_name = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Store results\n",
    "issue_skill_data = []\n",
    "\n",
    "#Here the prompts are self explanatory\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Extracting issue skills\"):\n",
    "    issue_id = row[\"issue_id\"]\n",
    "    issue_title = str(row.get(\"issue_title\", \"\"))\n",
    "    issue_body = str(row.get(\"issue_body\", \"\"))\n",
    "\n",
    "    system_prompt = \"You are an AI assistant that identifies technical skills needed to solve GitHub issues using a predefined skill list.\"\n",
    "\n",
    "    user_prompt = f\"\"\"Here is a list of allowed canonical technical skills:\n",
    "{baseline_block}\n",
    "\n",
    "Analyze the following GitHub issue and return only the relevant skills from the list above.\n",
    "\n",
    "Issue Title:\n",
    "{issue_title}\n",
    "\n",
    "Issue Description:\n",
    "{issue_body}\n",
    "\n",
    "Return only the skills in bullet points format. No extra explanations.\n",
    "\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    chat_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(chat_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=4096,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    skill_lines = response_text[len(chat_prompt):].strip().splitlines()\n",
    "\n",
    "    extracted_skills = []\n",
    "    for line in skill_lines:\n",
    "        if line.strip().startswith(\"-\"):\n",
    "            skill = line.strip().lstrip(\"-•* \").strip()\n",
    "            if skill:\n",
    "                extracted_skills.append(skill)\n",
    "\n",
    "    if extracted_skills:  # Only include rows with extracted skills\n",
    "        issue_skill_data.append({\n",
    "            \"issue_id\": issue_id,\n",
    "            \"required_skills\": \", \".join(sorted(set(extracted_skills)))\n",
    "        })\n",
    "\n",
    "# Save final output\n",
    "pd.DataFrame(issue_skill_data).to_csv(\"selective_issue_skills.csv\", index=False)\n",
    "print(f\"\\nSaved filtered issue skill dataset to 'selective_issue_skills.csv' with {len(issue_skill_data)} issues.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eca8c19",
   "metadata": {},
   "source": [
    "### Now for skill matching in the main approach the LLM performed very poor. So, here we are using TF-IDF and s-BERT only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7577f873-b285-4316-bda0-99534ce0e34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Evaluating top-k: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 127/127 [00:00<00:00, 1302.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-15 Accuracy: 61.42% (78/127 correct)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load files\n",
    "issue_df = pd.read_csv(\"selective_issue_skills.csv\")\n",
    "contrib_df = pd.read_csv(\"selective_contributor_skills_filtered.csv\")\n",
    "dataset_df = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "# Build ground-truth: issue_id → contributor_id\n",
    "issue_to_true_contrib = dataset_df.dropna(subset=[\"issue_id\", \"contributor_id\"]) \\\n",
    "    .drop_duplicates(\"issue_id\")[[\"issue_id\", \"contributor_id\"]] \\\n",
    "    .set_index(\"issue_id\")[\"contributor_id\"].to_dict()\n",
    "\n",
    "# TF-IDF model\n",
    "all_docs = list(issue_df[\"required_skills\"].astype(str)) + list(contrib_df[\"skills\"].astype(str))\n",
    "vectorizer = TfidfVectorizer(tokenizer=lambda x: [s.strip().lower() for s in x.split(\",\")])\n",
    "tfidf_matrix = vectorizer.fit_transform(all_docs)\n",
    "\n",
    "# Split vectors\n",
    "issue_vectors = tfidf_matrix[:len(issue_df)]\n",
    "contrib_vectors = tfidf_matrix[len(issue_df):]\n",
    "\n",
    "# Accuracy tracking\n",
    "top_k = 15\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for i, issue_row in tqdm(issue_df.iterrows(), total=len(issue_df), desc=\"Evaluating top-k\"):\n",
    "    issue_id = issue_row[\"issue_id\"]\n",
    "    if issue_id not in issue_to_true_contrib:\n",
    "        continue\n",
    "\n",
    "    true_contributor = issue_to_true_contrib[issue_id]\n",
    "    issue_vec = issue_vectors[i]\n",
    "    scores = cosine_similarity(issue_vec, contrib_vectors)[0]\n",
    "    top_k_indices = scores.argsort()[::-1][:top_k]\n",
    "    top_contrib_ids = [contrib_df.iloc[j][\"contributor_id\"] for j in top_k_indices]\n",
    "\n",
    "    if true_contributor in top_contrib_ids:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "accuracy = correct / total if total > 0 else 0\n",
    "\n",
    "print(f\"\\nTop-{top_k} Accuracy: {accuracy:.2%} ({correct}/{total} correct)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7b0207",
   "metadata": {},
   "source": [
    "### Now s-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d88d1dd-45fd-45de-a78b-8acbfba23684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 65.54it/s]\n",
      "Batches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 45.73it/s]\n",
      "Evaluating Top-k: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 127/127 [00:00<00:00, 4149.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-15 Accuracy using s-BERT: 56.69% (72/127 correct)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load data\n",
    "issue_df = pd.read_csv(\"selective_issue_skills.csv\")\n",
    "contrib_df = pd.read_csv(\"selective_contributor_skills.csv\")\n",
    "dataset_df = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "# Build ground truth: issue_id → contributor_id\n",
    "issue_to_true_contrib = dataset_df.dropna(subset=[\"issue_id\", \"contributor_id\"]) \\\n",
    "    .drop_duplicates(\"issue_id\")[[\"issue_id\", \"contributor_id\"]] \\\n",
    "    .set_index(\"issue_id\")[\"contributor_id\"].to_dict()\n",
    "\n",
    "# s-BERT model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Prepare texts\n",
    "issue_texts = issue_df[\"required_skills\"].astype(str).tolist()\n",
    "contrib_texts = contrib_df[\"skills\"].astype(str).tolist()\n",
    "contrib_ids = contrib_df[\"contributor_id\"].tolist()\n",
    "\n",
    "# Encode embeddings\n",
    "issue_embeddings = model.encode(issue_texts, convert_to_tensor=True, show_progress_bar=True)\n",
    "contrib_embeddings = model.encode(contrib_texts, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "# Evaluate top-k accuracy\n",
    "top_k = 15\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for i, issue_row in tqdm(issue_df.iterrows(), total=len(issue_df), desc=\"Evaluating Top-k\"):\n",
    "    issue_id = issue_row[\"issue_id\"]\n",
    "    if issue_id not in issue_to_true_contrib:\n",
    "        continue\n",
    "\n",
    "    true_contributor = issue_to_true_contrib[issue_id]\n",
    "    scores = util.cos_sim(issue_embeddings[i], contrib_embeddings)[0]\n",
    "    top_k_indices = torch.topk(scores, k=top_k).indices.tolist()\n",
    "    top_k_ids = [contrib_ids[j] for j in top_k_indices]\n",
    "\n",
    "    if true_contributor in top_k_ids:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "accuracy = correct / total if total > 0 else 0\n",
    "\n",
    "print(f\"\\nTop-{top_k} Accuracy using s-BERT: {accuracy:.2%} ({correct}/{total} correct)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd25c237-f3ec-4759-90e3-e8722cba33ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
